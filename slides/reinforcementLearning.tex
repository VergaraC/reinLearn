\documentclass[landscape,pdftex]{jomislides}

\slidesmag{5} % escala, qto maior maiores serão as letras/figras/etc.

%\centerslidesfalse

\usepackage{algorithmic}

%
% Slides
% ======
%


\begin{document}

%\input{autorHeaders}

\title{Aprendizagem por Reforço} 
\author{Fabrício Barth}
\institution{}
\date{Setembro de 2021}

\SlideHeader{}
            {Disciplina de Inteligência Artificial}
\SlideFooter{\theslidepartheading $\;$ --- $\;$ \theslideheading}
            { \theslide}

\vpagecolor[white]{white}


\subtitle{}

\maketitle

\begin{Slide}{Contexto}
Até o momento vimos nesta disciplina:
\begin{itemize}
	\item Conceito de Agente Autônomo;
	\item Solução de problemas usando busca em espaço de estados:
	\begin{itemize}
		\item Algoritmos de busca cega, e;
		\item Algoritmos de busca informados.
	\end{itemize}
	\item Busca competitiva.
\end{itemize}
\end{Slide}

\begin{Slide}{Conteúdo desta aula}
  \begin{itemize}
\item Visão Geral sobre Aprendizagem por Reforço
\item Algoritmo Q-Learning
\item Implementações com o projeto \textsc{Gym}
\item Considerações Finais
\item Material de Consulta
  \end{itemize}
\end{Slide}

\begin{Slide}{Ao final desta aula você saberá}

	\begin{itemize}
	\item o que é \textbf{Aprendizagem por Reforço} e como as suas principais ideias funcionam;
	\item como o algoritmo \textbf{Q-Learning} funciona e como implementá-lo;
	\item como implementar um \textbf{agente autônomo} usando aprendizagem por reforço, e;
	\item como implementar um agente autônomo para atuar nos ambientes do \textbf{projeto \textsc{Gym}}.
	\end{itemize}

\end{Slide}


\begin{Slide}{Visão Geral}
	\small
	Um agente aprende a resolver uma tarefa através de repetidas interações com o ambiente, por tentativa e erro, recebendo 
	(esporadicamente) reforços (punições ou recompensas) como retorno.
  \begin{center}
	\includegraphics[width=.7\textwidth]{figuras/visaoGeral.png}
\end{center}
\end{Slide}

\begin{Slide}{Visão Geral}
	\begin{itemize}
	\item Este agente não tem conhecimento algum sobre a tarefa que precisa executar (heurísticas ou funções de utilidade específicas).
	\item A \textbf{tarefa} deste agente é executar uma \textbf{sequência de ações}, observar as suas \textbf{consequências} e aprender uma \textbf{política de controle}.
	
	\end{itemize}
\end{Slide}


\begin{Slide}{Política de Controle}
	\begin{itemize}
	\item A política de controle desejada é aquela que \textbf{maximiza} os reforços 
	(\textit{reward}) acumulados ao longo do tempo pelo 
	agente: $r_{0} + \gamma r_{1} + \gamma^{2} r_{2} + \cdots $ onde $0 \leq \gamma < 1$.
	\end{itemize}

%\hspace{1cm}

  \begin{center}
	\includegraphics[width=.7\textwidth]{figuras/sequencia.png}
\end{center}

\begin{itemize}
	\item O $V(s_{1})$ será a soma de $r_{1}$ com o $V(s_{2})$. No entanto, considerando 
	o fator de desconto $\gamma$, temos: $V(s_{1}) = r_{1} + \gamma V(s_{2}) $.
	
	\item O valor de um estado final leva-se em consideração apenas o 
	reforço: $V(s_{n}) = r_{n}$.
	
	%\item Resumindo, temos: $V(s) = \sum_{t} \gamma^{t} R$
\end{itemize}
\end{Slide}

\begin{Slide}{Fator de desconto $\gamma$}
	\small
	\begin{itemize}
		%\item Para tarefas episódicas, o retorno é fácil de ser calculado, 
		%pois será a soma de todas as recompensas obtidas pelo agente. 
		%Mas para tarefas contínuas, como a atividade não tem fim e não podemos 
		%somar até o infinito, há a necessidade da inserção de um fator de 
		%desconto ($\gamma$).
		\item O fator de desconto ($\gamma$) é um hiperparâmetro que consiste 
		em um número entre 0 e 1 que define a importância das recompensas futuras 
		em relação a atual	($0 \leq \gamma < 1$).
		\item Valores mais próximos ao 0 dão mais importância a recompensas 
		imediatas enquanto os mais próximos de 1 tentarão manter a importância 
		de recompensas futuras.
	\end{itemize}
\end{Slide}


\begin{Slide}{Exemplo}
	\begin{center}
		\begin{tabular}{ |c|c|c|c| } 
			\hline
			\cellcolor{red!25} Início & Campo & Campo & Campo \\ 
			\hline
			Campo & \cellcolor{black!25} Buraco & Campo &  \cellcolor{black!25} Buraco\\ 
			\hline
			 Campo & Campo & Campo & \cellcolor{black!25} Buraco\\ 
			\hline
			\cellcolor{black!25} Buraco & Campo & Campo & \textbf{Objetivo} \\ 
			\hline
		\end{tabular}
	\end{center}

\vspace{0.2cm}
\small
\textbf{Ações}: 
\begin{itemize}
	\item (0) Mover para Baixo; (1)	Mover para Cima; 
	\item (2) Move para Direita; (3) Move para Esquerda.
\end{itemize}

\newpage

\begin{itemize}
	\item Considerando que o local do objetivo, dos buracos e dos campos serão sempre 
	os mesmos então temos \textbf{16} estados possíveis. 
	\item Este problema tem \textbf{4} ações possíveis. 
	\item Se o agente cair em um buraco ele recebe $-1$ como recompensa, se ele ir para um 
	campo ele recebe $0$ e ao chegar no objetivo ele recebe $1$.  
	\item Para que agente possa identificar uma política de controle ótima este agente 
	precisa criar um \textbf{mapeamento} entre \textbf{estados} (S) e \textbf{ações} (A)
	
	\newpage
	
	\item Este mapeamento pode ser representado por uma função $Q(S,A)$ onde $S$ são 
	todos os estados possíveis ($s_{1}, s_{2}, \cdots$) e onde $A$ são todas as 
	ações possíveis ($a_{1}, a_{2}, \cdots$)

\end{itemize}

	\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		 \textbf{Q-table}  & $a_{1}$ & $a_{2}$ & $a_{3}$& $a_{4}$ \\
		 \hline
		$s_{1}$&  &  &  & \\ 
		\hline
		$s_{2}$&  &  &  & \\ 
\hline
		$\cdots$&  &  &  & \\ 
\hline
		$s_{n}$&  &  &  & \\ 
\hline
	\end{tabular}
\end{center}

\begin{itemize}
	\item Para criar um \textbf{mapeamento} $Q(S,A)$ é necessário executar o 
	agente no ambiente considerando o \textbf{reforço} dado por cada ação. 
\end{itemize}

\newpage

  \begin{center}
	\includegraphics[width=\textwidth]{figuras/mapa1.png}
\end{center}

\newpage

\begin{itemize}
%	\item Qual é a recompensa do agente se ele seguir os seguintes caminhos? 
%	\begin{itemize}
%		\item $baixo \rightarrow baixo \rightarrow baixo \rightarrow esquerda \rightarrow esquerda \rightarrow esquerda \rightarrow direita$
%		\item $baixo \rightarrow direita \rightarrow baixo \rightarrow esquerda \rightarrow esquerda \rightarrow baixo $
%	\end{itemize}
	\item Como é que o agente pode saber quais são as melhores ações em cada estado? 
\end{itemize}
	
\end{Slide}

\begin{Slide}{Algoritmo Q-Learning}
	\begin{itemize}
		\item A ideia é fazer com que o agente aprenda a função de mapeamento $Q(S,A)$. 
		Ou seja, que seja capaz de identificar qual é a melhor ação para cada estado 
		através das suas \textbf{experiências}. 
		\item \textit{Testando} \textbf{infinitas} vezes o ambiente. 
		Ou seja, \textit{testando} \textbf{muitas} vezes as combinações entre 
		\textbf{estados} ($S$) e \textbf{ações} ($A$). 
	\end{itemize}

\newpage

\begin{small}
\begin{center}
	\begin{tabular}{ |c|c|c|c| } 
		\hline
		\cellcolor{red!25} Início & Campo & Campo & Campo \\ 
		\hline
		Campo & \cellcolor{black!25} Buraco & Campo &  \cellcolor{black!25} Buraco\\ 
		\hline
		 Campo & Campo & Campo & \cellcolor{black!25} Buraco\\ 
		\hline
		\cellcolor{black!25} Buraco & Campo & Campo & \textbf{Objetivo} \\ 
		\hline
	\end{tabular}
\end{center}
\end{small}

Primeiro episódio ($\gamma = 0.9$):
\begin{eqnarray}
Q(s_{1}, baixo) \leftarrow  r + \gamma \max_{a'}{Q(s', a')} \nonumber \\
Q(s_{1}, baixo) \leftarrow  0 + 0.9 \times \max[0, 0, 0] \nonumber \\
Q(s_{2}, esquerda) \leftarrow  -1 + 0.9 \times \max[0, 0, 0,0] \nonumber \\
%Q(s_{n}, esquerda) \leftarrow  1 + 0.9 \times \max[0, 0, 0,0] \nonumber
\end{eqnarray}

\newpage

Q-table resultante da execução do $1^{o}$ episódio. 

	\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		\textbf{Q-table}  & $esquerda$ & $baixo$ & $direita$& $cima$ \\
		\hline
		$s_{1}$& 0  & 0 & 0 & 0\\ 
		\hline
		$s_{2}$& -1 & 0  & 0 & 0\\ 
		\hline
$s_{3}$& 0 & 0 & 0 & 0\\ 
\hline
$s_{4}$& 0 & 0  & 0  &  0\\ 
\hline
		$\cdots$& $\cdots$ & $\cdots$ & $\cdots$ &$\cdots$ \\ 
		\hline
		$s_{n}$& 1 & 0  & 0  & 0 \\ 
		\hline
	\end{tabular}
\end{center}

%\newpage
%
%Já na execução do $2^{o}$ episódio... 
%
%	\begin{center}
%	\begin{tabular}{ |c|c|c|c| } 
%		\hline
%		 Início & Campo & Campo & Campo \\ 
%		\hline
%		Campo & \cellcolor{black!25} Buraco & Campo (0.0) &  \cellcolor{black!25} Buraco\\ 
%		\hline
%		Campo & Campo (0.0) & \cellcolor{red!25}Campo & \cellcolor{black!25} Buraco (-0.1)\\ 
%		\hline
%		\cellcolor{black!25} Buraco & Campo & Campo (0.9) & \textbf{Objetivo} \\ 
%		\hline
%	\end{tabular}
%\end{center}

\newpage

Q-table resultante da execução do $n$-éssimo episódio. 

\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } 
		\hline
		\textbf{Q-table}  & $esquerda$ & $baixo$ & $direita$& $cima$ \\
		\hline
		$s_{1}$& 0.02  & 0.03 & 0.0001 & 0.0001\\ 
		\hline
		$s_{2}$& 0.03 & -0.003  & 0.05 & 0.001\\ 
		\hline
		$\cdots$& $\cdots$ & $\cdots$ & $\cdots$ &$\cdots$ \\ 
		\hline
		$s_{n}$& 0.985 & 0.0001  & 0.003  & 0.002 \\ 
		\hline
	\end{tabular}
\end{center}

Após a execução de $n$ episódios o agente conhece qual a melhor ação para cada estado. 

\end{Slide}


\begin{Slide}{Algoritmo Q-Learning } 
	\small
	\begin{algorithmic} 
		\STATE inicializar os valores de $Q(s, a)$ arbitrariamente
		\FOR {todos os episódios}
		\REPEAT
		\STATE escolher uma ação $a$ para um estado $s$
		\STATE executar a ação $a$
		\STATE observar a recompensa $r$ e o novo estado $s'$ 
		\STATE $Q(s,a) \leftarrow Q(s,a) + \alpha [r +\gamma \max_{a'}{Q(s', a')} - Q(s,a)]$
		\STATE$s  \leftarrow s'$
		\UNTIL {$s$ ser um estado final}
		\ENDFOR
		\STATE \textbf{return} $Q(s, a)$
	\end{algorithmic}
\end{Slide}

\begin{Slide}{Algoritmo Q-Learning: hiperparâmetro $\alpha$}
	\begin{itemize}
		\item $\alpha$ é a taxa de aprendizado ($0 < \alpha \leq 1$), quanto maior, 
		mais valor dá ao novo aprendizado.
	\end{itemize}
\end{Slide}

\begin{Slide}{\textit{Exploration} vs \textit{Exploitation}}
	\begin{itemize}
		\item A política que o agente utiliza para escolher 
		uma ação $a$ para um estado $s$ não interfere no aprendizado 
		da \textit{Q-table}.
		\item No entanto, para que o algoritmo \textit{Q-learning} possa convergir 
		para um determinado problema é necessário que o algoritmo visite pares de 
		ação-estado infinitas (muitas) vezes.
		\item Por isso, que a escolha de determinada \textit{ação} em um \textit{estado} 
		poderia ser feita de forma \textbf{aleatória}. 
		
		\newpage
		
		\item Porém, normalmente se utiliza uma política que inicialmente escolhe 
		aleatoriamente as ações, e, à medida que vai aprendendo, passa a utilizar cada 
		vez mais as decisões determinadas pela política derivada de $Q$. 
		
		\item Esta estratégia inicia \textbf{explorando} (tentar uma ação mesmo que ela não 
		tenha o maior valor de $Q$) e termina escolhendo a ação que tem o 
		maior valor de $Q$ (\textit{exploitation}).  
		
	\end{itemize}
\end{Slide}


\begin{Slide}{Exemplo de função para escolha de ações}
	
	A escolha de uma ação para um estado é dada pela função:
	
	\vspace{0.3cm}
	
	\begin{algorithmic} 
		\STATE \textbf{function} escolha($s$, $\epsilon$): $a$
		\STATE rv = random ($0 < rv \leq 1$)
		\IF{$rv < \epsilon$}
		\STATE \textbf{return} uma ação $\alpha$ aleatória em $A$
		\ENDIF   
		\STATE \textbf{return} $\max_{a}{Q(s, a)} $
	\end{algorithmic}

	\vspace{0.3cm}
	
	O fator de exploração $\epsilon$ ($0 \leq \epsilon \leq 1$) inicia com um valor 
	alto ($0.7$, por exemplo) e, conforme a simulação avança, 
	diminiu: $\epsilon \leftarrow \epsilon \times \epsilon_{dec}$, 
	onde $\epsilon_{dec} = 0.99$
	
\end{Slide}

\begin{Slide}{Epsilon}
	  \begin{center}
		\includegraphics[width=\textwidth]{figuras/epsilon.png}
	\end{center}
\end{Slide}

\begin{Slide}{Algoritmo Q-Learning}
	\small
\begin{algorithmic} 
	\STATE \textbf{function} Q-Learning(env, $\alpha$, $\gamma$, $\epsilon$, $\epsilon_{min}$, $\epsilon_{dec}$, episódios)
	\STATE inicializar os valores de $Q(s, a)$ arbitrariamente
	\STATE inicializar $s$ a partir de $env$
	\FOR {todos os episódios}
	\REPEAT
	\STATE $a \leftarrow escolha(s, \epsilon)$
	\STATE $s', r \leftarrow$ executar a ação $a$ no $env$
	\STATE $Q(s,a) \leftarrow Q(s,a) + \alpha [r +\gamma \max_{a'}{Q(s', a')} - Q(s,a)]$
	\STATE$s  \leftarrow s'$
	\UNTIL {$s$ ser um estado final}
	\STATE \textbf{if} $\epsilon > \epsilon_{min}$ \textbf{then} $\epsilon \leftarrow \epsilon \times \epsilon_{dec}$
	\ENDFOR
	\STATE \textbf{return} Q
\end{algorithmic}	
\end{Slide}

\begin{Slide}{Implementações com o projeto \textsc{Gym}}
	\small
	\begin{itemize}
		\item Siga as orientações que estão no arquivo README.md da 
		pasta \href{https://github.com/fbarth/reinLearn}.
		
		\item Execute as atividades que estão descritas no arquivo Atividades.md no 
		mesmo diretório (\href{https://github.com/fbarth/reinLearn/atividades_01.md}). 
	\end{itemize}
\end{Slide}

%\begin{Slide}{Q-Learning com GridSearch}
%Faz sentido usar GridSearch para encontrar os melhores valores de $\alpha$ e $\gamma$? 
%\end{Slide}

\begin{Slide}{Considerações Finais}
\begin{itemize}
	\item O algoritmo \textit{Q Learning} pode ser utilizado por agentes que 
	não tem conhecimento prévio sobre o problema.
	
	\item Diversos autores já provaram que o algoritmo \textit{Q Learning} converge 
	para a função correta $Q$ dentro de certas condições. Por exemplo, uma 
	delas é garantir que o agente avalie um par $Q(s,a)$ diversas vezes.  
	
	\newpage
	
	\item \textit{Q Learning} converge tanto para processos de decisão de Markov 
	(MDP) \textbf{determinísticos} e \textbf{não-determinísticos}.
	
	\item Na prática, o algoritmo \textit{Q Learning} necessita de muitas iterações de 
	treinamento até convergir, inclusive para problemas que não tem um espaço de busca 
	tão grande.   

	\newpage

	\item E quando o espaço de estados for muito grande? 
	
	\newpage

	\item Usar \textit{Deep Learning} com \textit{Reinforcement Learning}! \emph{Assunto do 
	nosso próximo encontro!}

	\item Ler o capítulo \textit{18. Reinforcement Learning} até a seção 
	\textit{Implementing Deep Q-Learning} do livro \textbf{Hands-On Machine Learning 
	with Scikit-Learn, Keras, and TensorFlow, 2nd Edition} do Aurélien Géron.

\end{itemize}
\end{Slide}

\begin{Slide}{O que vimos até o momento?}
\begin{itemize}
	\item Conceitos básicos de Aprendizagem por Reforço.
	\item Funcionamento do algoritmo Q-Learning.
	\item Funcionamento da biblioteca Gym. 
\end{itemize}
\end{Slide}

\begin{Slide}{Material de \textbf{consulta}}
	\small
  \begin{itemize}
  	\item Tom Mitchell. Machine Learning. McGraw-Hill, 1997.
  	\item Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. Second Edition, in progress. The MIT Press, 2015.
  \item Projeto Gym: https://gym.openai.com/
  \item https://deepmind.com/research/case-studies/alphago-the-story-so-far
  \item Aurélien Géron. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, 2019. 
  \end{itemize}
\end{Slide}


\end{document}

